{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installations and Environment Setup"
      ],
      "metadata": {
        "id": "xt1p1mP9Yg8o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ahhDKRT-y8l2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets accelerate"
      ],
      "metadata": {
        "collapsed": true,
        "id": "KwQ9zdABEwtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate"
      ],
      "metadata": {
        "collapsed": true,
        "id": "gktYH0Ncx3vL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jiwer"
      ],
      "metadata": {
        "collapsed": true,
        "id": "GdFsvbybx8rr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing necessary libraries\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install jiwer datasets transformers evaluate torch torchaudio  accelerate\n",
        "!pip install torch --upgrade --extra-index-url https://download.pytorch.org/whl/cu118\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "YD2zwpmvySXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "import zipfile\n",
        "import requests\n",
        "import io\n",
        "import re\n",
        "import locale\n",
        "import shutil\n",
        "\n",
        "# Data handling and processing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Audio processing\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "\n",
        "# Transformers library\n",
        "from transformers import (\n",
        "    PreTrainedTokenizerFast,\n",
        "    WhisperForConditionalGeneration,\n",
        "    WhisperProcessor,\n",
        "    WhisperTokenizer,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer,\n",
        "    EarlyStoppingCallback,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "\n",
        "# Datasets and utilities\n",
        "from datasets import Dataset, load_dataset, Audio\n",
        "from torch.utils.data import Dataset as TorchDataset, DataLoader\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Evaluation and metrics\n",
        "import evaluate\n",
        "from evaluate import load\n",
        "import jiwer\n",
        "from jiwer import wer\n",
        "from whisper.normalizers import EnglishTextNormalizer\n",
        "\n",
        "\n",
        "# IPython display\n",
        "from IPython.display import Audio, display, HTML\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nested\n",
        "\n",
        "\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "from pathlib import Path\n"
      ],
      "metadata": {
        "id": "krhlPC9W7uGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Twi Bible Dataset"
      ],
      "metadata": {
        "id": "SO9TBrPNX8Aa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "7URXiOFAxpKQ"
      },
      "outputs": [],
      "source": [
        "ds = load_dataset(\"kojo-george/asante-twi-tts\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_file_path = Path(\"asante_twi_tts_train.txt\")\n",
        "test_file_path = Path(\"asante_twi_tts_test.txt\")\n",
        "val_file_path = Path(\"asante_twi_tts_val.txt\")\n",
        "\n",
        "# Extracting text from the dataset and saving to files\n",
        "with train_file_path.open(\"w\") as f:\n",
        "    for item in ds[\"train\"]:\n",
        "        f.write(item[\"text\"] + \"\\n\")\n",
        "\n",
        "with test_file_path.open(\"w\") as f:\n",
        "    for item in ds[\"test\"]:\n",
        "        f.write(item[\"text\"] + \"\\n\")\n",
        "\n",
        "with val_file_path.open(\"w\") as f:\n",
        "    for item in ds[\"validation\"]:\n",
        "        f.write(item[\"text\"] + \"\\n\")\n",
        "\n",
        "train_save_path = '/content/drive/MyDrive/datasets/asante_twi_train.txt'\n",
        "test_save_path = '/content/drive/MyDrive/datasets/asante_twi_test.txt'\n",
        "val_save_path = '/content/drive/MyDrive/datasets/asante_twi_val.txt'\n",
        "\n",
        "\n",
        "shutil.copy(str(train_file_path), train_save_path)\n",
        "shutil.copy(str(test_file_path), test_save_path)\n",
        "shutil.copy(str(val_file_path), val_save_path)"
      ],
      "metadata": {
        "id": "MCd15Ujk82b8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Tokenizer on Dataset and converting to a type compatible with HuggingFace\n"
      ],
      "metadata": {
        "id": "LAZFDDjLYEVB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the tokenizer\n",
        "tokenizer = ByteLevelBPETokenizer()\n",
        "\n",
        "# Training the tokenizer\n",
        "tokenizer.train(\n",
        "    files=[str(train_file_path), str(test_file_path), str(val_file_path)],\n",
        "    vocab_size=12000, # 12000 because the data itself has just about 28000 rows\n",
        "    min_frequency=2, # smaller minimum frequency because of the smaller size of data\n",
        "    special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"],\n",
        ")\n",
        "\n",
        "save_path = '/content/drive/MyDrive/tokenizers/asante_twi_jw_tokenizer'\n",
        "tokenizer.save_model(save_path)"
      ],
      "metadata": {
        "id": "oOmxB_nh_v1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Had a lot of erros using the tokenizer, so converting it to a PreTrainedTokenizerFast type per recommendation\n",
        "class CustomTokenizer(PreTrainedTokenizerFast):\n",
        "    def __init__(self, tokenizer_file, **kwargs):\n",
        "          tokenizer_object=ByteLevelBPETokenizer.from_file(\n",
        "              os.path.join(tokenizer_file, \"vocab.json\"),\n",
        "              os.path.join(tokenizer_file, \"merges.txt\"))\n",
        "          super().__init__(\n",
        "          tokenizer_object=tokenizer_object,\n",
        "          bos_token=\"<s>\", # Adding necessary attributes for PreTrainedTokenizerFast\n",
        "          eos_token=\"</s>\",\n",
        "          unk_token=\"<unk>\",\n",
        "          pad_token=\"<pad>\",\n",
        "          mask_token=\"<mask>\",\n",
        "          **kwargs # other attributes\n",
        "          )\n",
        "          self.tokenizer_object = tokenizer_object\n",
        "\n",
        "# Loading the tokenizer using the custom class\n",
        "tokenizer_file = \"/content/drive/MyDrive/tokenizers/asante_twi_jw_tokenizer\"\n",
        "tokenizer = CustomTokenizer(tokenizer_file)\n",
        "\n",
        "save_path = '/content/drive/MyDrive/tokenizers/atjt2'\n",
        "tokenizer.save_pretrained(save_path)\n",
        "\n",
        "newsavepath = Path(\"/content/drive/MyDrive/tokenizers/atjt2_fixed\")\n",
        "#newsavepath.mkdir(exist_ok=True)\n",
        "tokenizer.tokenizer_object.save_model(str(newsavepath))"
      ],
      "metadata": {
        "id": "BqYgwMxQgDp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using functions from finetuning Whisper on the Financial Inclusion Data Only"
      ],
      "metadata": {
        "id": "eUdVu27Lp_l2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading Tokenizer\n",
        "tokenizer = CustomTokenizer(\"/content/drive/MyDrive/tokenizers/atjt2_fixed\")\n",
        "\n",
        "# Loading the Whisper processor\n",
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\", language=\"ak\", task=\"transcribe\") # ak is the language code for Akan\n",
        "processor.tokenizer = tokenizer #setting the tokenizer to my custom tokenizer\n"
      ],
      "metadata": {
        "id": "TnxKJHseqekF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(batch):\n",
        "    # loading and resampling audio data from 48 to 16kHz\n",
        "    audio = batch[\"audio\"]\n",
        "    # computing log-Mel input features from input audio array\n",
        "    batch[\"input_features\"] = processor.feature_extractor(audio[\"array\"], sampling_rate=16000, return_tensors = \"pt\").input_features[0]\n",
        "    # encoding target text to label ids\n",
        "    batch[\"labels\"] = processor(text=batch[\"text\"]).input_ids\n",
        "    return batch\n",
        "\n",
        "#trying to make a copy of the dataset so if it corrupts I don't have to rerun that whole part since it takes forever\n",
        "'''modified_ds = ds.copy()\n",
        "\n",
        "# Cast the \"audio\" column to Audio with the desired sampling rate\n",
        "modified_ds = modified_ds.map(\n",
        "    lambda example: {\"audio\": example[\"audio\"].cast_column(\"audio\", Audio(sampling_rate=16000))},\n",
        "    batched=False\n",
        ")'''\n",
        "ds = ds.cast_column(\"audio\", Audio(sampling_rate = 16000)) #resampling\n",
        "dsnew = ds.map(prepare_dataset, remove_columns=ds.column_names[\"train\"], num_proc=4)"
      ],
      "metadata": {
        "id": "KFUbIFSrGY-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6F_j0qpoMZxO"
      },
      "outputs": [],
      "source": [
        "def prepare_model_and_dataset(customprocessor):\n",
        "    # Loading Whisper model and processor\n",
        "    processor = customprocessor\n",
        "    model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.config.forced_decoder_ids = None  # Reseting forced decoder IDs\n",
        "    model.config.suppress_tokens = []  # Reseting suppressed tokens\n",
        "    model.config.vocab_size = tokenizer.vocab_size  # Updating vocab size\n",
        "\n",
        "    train_dataset = ds[\"train\"]\n",
        "    test_dataset = ds[\"test\"]\n",
        "    val_dataset = ds[\"validation\"]\n",
        "\n",
        "    return model, processor, train_dataset, test_dataset\n",
        "\n",
        "print(\"Data prepared and model loaded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkPgty6wMdJU"
      },
      "outputs": [],
      "source": [
        "def prepare_training_args(output_dir='content/drive/MyDrive/model_with_tokenizer'):\n",
        "    training_args = Seq2SeqTrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=10,\n",
        "        per_device_train_batch_size=4,\n",
        "        per_device_eval_batch_size=4,\n",
        "\n",
        "        # Learning rate scheduling and regularization\n",
        "        learning_rate=1e-4,  # Small learning rate to prevent rapid overfitting\n",
        "        weight_decay=0.01,   # L2 regularization to penalize large weights\n",
        "        lr_scheduler_type=\"linear\",  # Gradual learning rate reduction\n",
        "        warmup_steps=500,    # Gradual learning rate increase initially\n",
        "\n",
        "        # Model saving and evaluation\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"wer\",  # Using Word Error Rate for model selection\n",
        "\n",
        "\n",
        "        gradient_accumulation_steps=2,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        predict_with_generate=True,\n",
        "        generation_max_length=225,\n",
        "        fp16=True,\n",
        "        report_to=[]\n",
        "    )\n",
        "    return training_args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rEs0KVdLPzJo"
      },
      "outputs": [],
      "source": [
        "def data_collator(batch):\n",
        "    required_length = 3000  # Fixed length required by Whisper\n",
        "\n",
        "    # Pad or truncate input features\n",
        "    input_features = torch.stack([\n",
        "        torch.nn.functional.pad(\n",
        "            item[\"input_features\"],\n",
        "            (0, required_length - item[\"input_features\"].shape[-1]),  # Pad to the right\n",
        "            mode=\"constant\",\n",
        "            value=0  # Padding value\n",
        "        )[:, :required_length]  # Truncate if longer than 3000\n",
        "        for item in batch\n",
        "    ])\n",
        "\n",
        "    # Pad labels to the maximum sequence length in the batch\n",
        "    labels = torch.nn.utils.rnn.pad_sequence(\n",
        "        [item[\"labels\"] for item in batch],\n",
        "        batch_first=True,\n",
        "        padding_value=processor.tokenizer.pad_token_id\n",
        "    )\n",
        "    labels[labels == processor.tokenizer.pad_token_id] = -100  # Mask padding tokens for loss calculation\n",
        "\n",
        "    return {\n",
        "        \"input_features\": input_features,  # Padded to required length\n",
        "        \"labels\": labels\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6XFMjgwMiGU"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(pred):\n",
        "    # pred contains predictions and label_ids\n",
        "    decoded_preds = processor.batch_decode(pred.predictions, skip_special_tokens=True)\n",
        "    decoded_labels = processor.batch_decode(pred.label_ids, skip_special_tokens=True)\n",
        "\n",
        "    # Normalizing text\n",
        "    normalizer = EnglishTextNormalizer()\n",
        "    decoded_preds_clean = [normalizer(text) for text in decoded_preds]\n",
        "    decoded_labels_clean = [normalizer(text) for text in decoded_labels]\n",
        "\n",
        "    # Computing WER\n",
        "    wer = jiwer.wer(decoded_labels_clean, decoded_preds_clean)\n",
        "\n",
        "    # Computing CER\n",
        "    cer = jiwer.cer(decoded_labels_clean, decoded_preds_clean)\n",
        "\n",
        "    # Print the results (optional)\n",
        "    print(f\"WER: {wer * 100:.2f} %\")\n",
        "    print(f\"CER: {cer * 100:.2f} %\")\n",
        "\n",
        "    return {\"wer\": wer, \"cer\": cer}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwx-TuJ3MmwP"
      },
      "outputs": [],
      "source": [
        "def train_whisper_model():\n",
        "    #Using a smaller Whisper model to reduce complexity\n",
        "    model, processor, train_dataset, test_dataset = prepare_model_and_dataset(processor)\n",
        "\n",
        "    #Freezing initial layers of the model\n",
        "    for param in model.base_model.parameters():\n",
        "        param.requires_grad = False  # Freeze initial layers\n",
        "\n",
        "    # Unfreezing last few layers for fine-tuning\n",
        "    for param in model.base_model.encoder.layers[-2:].parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    training_args = prepare_training_args()\n",
        "\n",
        "    # Custom data collator for batching to pad appropriately\n",
        "    def collate_fn(batch):\n",
        "         # Filter out None values\n",
        "        batch = [b for b in batch if b is not None]\n",
        "        if len(batch) == 0:\n",
        "            # Handling the case where all items in the batch are None\n",
        "            return {}  # or return some default value\n",
        "\n",
        "        input_features = torch.stack([x[0] for x in batch])\n",
        "        labels = torch.nn.utils.rnn.pad_sequence(\n",
        "            [x[1] for x in batch],\n",
        "            batch_first=True,\n",
        "            padding_value=processor.tokenizer.pad_token_id\n",
        "        )\n",
        "        return {\n",
        "            \"input_features\": input_features,\n",
        "            \"labels\": labels\n",
        "        }\n",
        "\n",
        "    trainer = Seq2SeqTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=test_dataset,\n",
        "        data_collator=collate_fn,\n",
        "        compute_metrics=compute_metrics,\n",
        "        processing_class=processor\n",
        "    )\n",
        "\n",
        "\n",
        "    # Adding early stopping callback --adding it earlier in the training arguments was causing errors\n",
        "    trainer.add_callback(EarlyStoppingCallback(early_stopping_patience=3))\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    # Save model\n",
        "    trainer.save_model('content/drive/MyDrive/asomsem')\n",
        "\n",
        "    return model, processor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute training\n",
        "trained_model, trained_processor = train_whisper_model()\n",
        "print(\"Model training completed!\")\n",
        "\n",
        "# Saving the model and tokenizer to Google Drive\n",
        "model_save_path = 'content/drive/MyDrive/asomsem'  # Path for model\n",
        "tokenizer_save_path = 'content/drive/MyDrive/asomsem_tokenizer'  # Path for tokenizer\n",
        "\n",
        "trained_model.save_pretrained(model_save_path)  # Save model\n",
        "trained_processor.save_pretrained(tokenizer_save_path)  # Save tokenizer\n",
        "\n",
        "print(f\"Model saved to: {model_save_path}\")\n",
        "print(f\"Tokenizer saved to: {tokenizer_save_path}\")"
      ],
      "metadata": {
        "id": "6TS9d07pw-qK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finetuning on the financial inclusion dataset"
      ],
      "metadata": {
        "id": "sCukjKF8LvND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import zipfile\n",
        "import requests\n",
        "import io\n",
        "import pandas as pd\n",
        "import torch\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "from transformers import RobertaConfig, RobertaTokenizerFast, RobertaForMaskedLM, Trainer, TrainingArguments ,DataCollatorForLanguageModeling\n",
        "from datasets import Dataset\n",
        "from torch.utils.data import Dataset as TorchDataset"
      ],
      "metadata": {
        "id": "Q0Q5O7x4L7tL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AsantiTwiDataset(TorchDataset):\n",
        "    def __init__(self, zip_url, csv_filename, audio_base_path, tokenizer=None):\n",
        "        self.tokenizer = tokenizer\n",
        "        # Download and extract dataset\n",
        "        response = requests.get(zip_url, stream=True)\n",
        "        response.raise_for_status()\n",
        "        with zipfile.ZipFile(io.BytesIO(response.content), 'r') as zip_ref:\n",
        "            zip_ref.extractall('.')\n",
        "\n",
        "        # Clean the CSV file\n",
        "        cleaned_csv_filename = f\"cleaned_{os.path.basename(csv_filename)}\"\n",
        "        self._clean_csv(csv_filename, cleaned_csv_filename)\n",
        "\n",
        "        # Load and preprocess data\n",
        "        self.df = pd.read_csv(cleaned_csv_filename)\n",
        "        self.df.rename(columns={\"Audio Filepath\": \"path\", \"Transcription\": \"sentence\"}, inplace=True)\n",
        "        self.texts = self.df[\"sentence\"].tolist()\n",
        "        self.audio_base_path = audio_base_path\n",
        "\n",
        "    def _clean_csv(self, input_path, output_path):\n",
        "        with open(input_path, \"r\") as infile:\n",
        "            lines = infile.readlines()\n",
        "\n",
        "        # Replace tabs with commas and clean paths\n",
        "        clean_lines = [\n",
        "            line.replace(\"\\t\", \",\")\n",
        "                .replace(\"lacuna-audios-train/asanti-twi/audios/\", \"\")\n",
        "                .replace(\"lacuna-audios-test/asanti-twi/audios/\", \"\")\n",
        "            for line in lines\n",
        "        ]\n",
        "\n",
        "        # Filter rows with the correct number of fields\n",
        "        expected_fields = clean_lines[0].count(\",\") + 1\n",
        "        valid_lines = [line for line in clean_lines if line.count(\",\") + 1 == expected_fields]\n",
        "\n",
        "        # Write cleaned content to a new file\n",
        "        with open(output_path, \"w\") as outfile:\n",
        "            outfile.writelines(valid_lines)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        audio_path = os.path.join(self.audio_base_path, self.df.iloc[idx]['path'])\n",
        "\n",
        "        # Load audio\n",
        "        waveform, sr = torchaudio.load(audio_path, normalize=True)\n",
        "        if sr != 16000:\n",
        "            resampler = T.Resample(orig_freq=sr, new_freq=16000)\n",
        "            waveform = resampler(waveform)\n",
        "\n",
        "        # Feature extraction (Mel Spectrogram with 80 Mel frequency bins)\n",
        "        mel_spectrogram = T.MelSpectrogram(n_mels=80)(waveform)  # Set n_mels=80 as expected by Whisper\n",
        "\n",
        "        # Ensure the correct shape [batch_size, n_mels, time]\n",
        "        mel_spectrogram = mel_spectrogram.squeeze(0)  # Remove channel dimension if it's 1\n",
        "\n",
        "        # Tokenize text to get labels\n",
        "        labels = self.tokenizer.encode(text, add_special_tokens=False)  # Directly get token IDs\n",
        "\n",
        "        return {\"input_features\": mel_spectrogram, \"labels\": labels}\n"
      ],
      "metadata": {
        "id": "ahoO0mk1L-XY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zip_url = \"https://fisd-dataset.s3.amazonaws.com/fisd-asanti-twi-90p.zip\"  # Training dataset URL\n",
        "csv_filename = \"fisd-asanti-twi-90p/data.csv\"\n",
        "audio_base_path = \"fisd-asanti-twi-90p/audios\"\n",
        "tokenizer_dir = \"content/drive/MyDrive/tokenizers/asante_twi_jw_tokenizer\"\n",
        "model_dir = \"content/drive/MyDrive/asomsem\"\n",
        "output_dir = \"content/drive/MyDrive/asomsem_financial\"\n",
        "\n",
        "# Step 1: Prepare tokenizer\n",
        "financial_train_dataset = AsantiTwiDataset(zip_url, csv_filename, audio_base_path)\n",
        "tokenizer = WhisperTokenizer.from_pretrained(tokenizer_dir)\n",
        "\n",
        "financial_inclusion_texts = financial_train_dataset.texts"
      ],
      "metadata": {
        "id": "_uTLrRK6PmPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import ByteLevelBPETokenizer\n",
        "\n",
        "def fine_tune_existing_tokenizer(texts, tokenizer_dir):\n",
        "    # Load the existing tokenizer from the directory\n",
        "    tokenizer = ByteLevelBPETokenizer.from_file(\n",
        "        os.path.join(tokenizer_dir, \"vocab.json\"),\n",
        "        os.path.join(tokenizer_dir, \"merges.txt\")\n",
        "    )\n",
        "\n",
        "    # Fine-tune the tokenizer on the new texts\n",
        "    tokenizer.train_from_iterator(\n",
        "        texts,\n",
        "        vocab_size=tokenizer.get_vocab_size(),  # Keep the original vocabulary size\n",
        "        min_frequency=2,     # Adjust as needed\n",
        "        special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"],\n",
        "    )\n",
        "\n",
        "    # Save the fine-tuned tokenizer\n",
        "    tokenizer.save_model(tokenizer_dir)\n",
        "\n",
        "    return tokenizer\n",
        "\n",
        "#financial_inclusion_texts = financial_train_dataset # Done earlier\n",
        "tokenizer = fine_tune_existing_tokenizer(financial_inclusion_texts, tokenizer_dir)"
      ],
      "metadata": {
        "id": "FkSkKgA4Nt_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare and split dataset\n",
        "def prepare_and_split_dataset(zip_url, csv_filename, audio_base_path, tokenizer):\n",
        "    dataset = AsantiTwiDataset(zip_url, csv_filename, audio_base_path, tokenizer=tokenizer)\n",
        "    texts = dataset.texts\n",
        "\n",
        "    # Create Hugging Face dataset\n",
        "    dataset_dict = {\"text\": texts}\n",
        "    huggingface_dataset = Dataset.from_dict(dataset_dict)\n",
        "\n",
        "    # Tokenize the dataset\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
        "\n",
        "    tokenized_dataset = huggingface_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "    # Split into train and validation sets\n",
        "    train_test_split = tokenized_dataset.train_test_split(test_size=0.2, seed=42)\n",
        "    train_dataset = train_test_split[\"train\"]\n",
        "    validation_dataset = train_test_split[\"test\"]\n",
        "\n",
        "    return train_dataset, validation_dataset"
      ],
      "metadata": {
        "id": "VwOkF7jPL_2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''from tokenizers import ByteLevelBPETokenizer\n",
        "\n",
        "def fine_tune_existing_tokenizer(texts, tokenizer_dir):\n",
        "    # Load the existing tokenizer from the directory\n",
        "    tokenizer = ByteLevelBPETokenizer.from_file(\n",
        "        os.path.join(tokenizer_dir, \"vocab.json\"),\n",
        "        os.path.join(tokenizer_dir, \"merges.txt\")\n",
        "    )\n",
        "\n",
        "    # Fine-tune the tokenizer on the new texts\n",
        "    tokenizer.train_from_iterator(\n",
        "        texts,\n",
        "        vocab_size=tokenizer.get_vocab_size(),  # Keep the original vocabulary size\n",
        "        min_frequency=2,     # Adjust as needed\n",
        "        special_tokens=[\"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"],\n",
        "    )\n",
        "\n",
        "    # Save the fine-tuned tokenizer\n",
        "    tokenizer.save_model(tokenizer_dir)\n",
        "\n",
        "    return tokenizer\n",
        "\n",
        "# Fine-tune your existing tokenizer\n",
        "tokenizer = fine_tune_existing_tokenizer(all_texts, tokenizer_dir)  # Use your current texts and tokenizer directory'''"
      ],
      "metadata": {
        "id": "3qNjVZj3Lzd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions I am not using at the moment"
      ],
      "metadata": {
        "id": "Gf70H1sovb5F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from datasets import load_dataset, Audio\n",
        "#from transformers import WhisperProcessor, WhisperForConditionalGeneration, TrainingArguments, Trainer\n",
        "#In case the above fails because  of the multiprocessing again\n",
        "# Load the tokenizer\n",
        "\n",
        "'''from tokenizers import ByteLevelBPETokenizer\n",
        "tokenizer_vocab_path = \"/content/drive/MyDrive/tokenizers/asante_twi_jw_tokenizer/vocab.json\"\n",
        "tokenizer_merges_path = \"/content/drive/MyDrive/tokenizers/asante_twi_jw_tokenizer/merges.txt\"\n",
        "\n",
        "def prepare_dataset(batch, tokenizer_vocab_path=tokenizer_vocab_path, tokenizer_merges_path=tokenizer_merges_path):\n",
        "    try:\n",
        "        # Re-initialize tokenizer and processor within the function\n",
        "        tokenizer = ByteLevelBPETokenizer.from_file(tokenizer_vocab_path, tokenizer_merges_path)\n",
        "        processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"ak\", task=\"transcribe\") # ak is the language code for Akan\n",
        "        processor.tokenizer = tokenizer\n",
        "\n",
        "        # load and resample audio data from 48 to 16kHz\n",
        "        audio = batch[\"audio\"]\n",
        "        # compute log-Mel input features from input audio array\n",
        "        batch[\"input_features\"] = processor.feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
        "        # encode target text to label ids\n",
        "        batch[\"labels\"] = processor.tokenizer.encode(batch[\"text\"]).ids\n",
        "        return batch\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing batch: {batch}\")  # Print the problematic batch\n",
        "        print(f\"Error: {e}\")\n",
        "        raise e\n",
        "\n",
        "ds = ds.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
        "ds = ds.map(prepare_dataset, remove_columns=ds.column_names[\"train\"], num_proc=1, fn_kwargs={'tokenizer_vocab_path': tokenizer_vocab_path, 'tokenizer_merges_path': tokenizer_merges_path})\n",
        "# Switch back to num_proc=4 once the error is resolved.'''"
      ],
      "metadata": {
        "id": "T83jLYvZZhPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer, padding=\"longest\", return_tensors=\"pt\")\n",
        "\n",
        "# Adjust data_collator's __call__ if needed for Whisper:\n",
        "def custom_data_collator_call(self, features):\n",
        "    input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "    #label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]  # Change 'labels' to 'input_ids'\n",
        "    input_features = self.tokenizer.pad(input_features, padding=self.padding, return_tensors=self.return_tensors)\n",
        "\n",
        "    # Process labels (as input_ids)\n",
        "    labels = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "    labels = self.tokenizer.pad(labels, padding=self.padding, max_length=None, pad_to_multiple_of=None, return_tensors=self.return_tensors)\n",
        "\n",
        "    # Combine padded features into a batch\n",
        "    batch = {\n",
        "        \"input_features\": input_features[\"input_features\"],\n",
        "        \"labels\": labels[\"input_ids\"],\n",
        "        \"attention_mask\": labels[\"attention_mask\"]\n",
        "    }\n",
        "\n",
        "    return batch\n",
        "\n",
        "data_collator.__call__ = custom_data_collator_call.__get__(data_collator, DataCollatorWithPadding)'''\n",
        "'''# The change is here. We pad the label_features with the tokenizer and pass the input_features as is.\n",
        "    # We also include the attention mask in label_features to make sure it is included in the output.\n",
        "    label_features = self.tokenizer.pad(label_features, padding=self.padding, return_tensors=self.return_tensors)\n",
        "\n",
        "    # Combine padded features into a batch\n",
        "    batch = {\n",
        "        \"input_features\": torch.tensor([feature[\"input_features\"] for feature in features]),\n",
        "        \"labels\": label_features[\"input_ids\"],\n",
        "        \"attention_mask\": label_features[\"attention_mask\"]  # add attention mask from label_features\n",
        "    }\n",
        "\n",
        "    return batch\n",
        "\n",
        "data_collator.__call__ = custom_data_collator_call.__get__(data_collator, DataCollatorWithPadding)'''\n"
      ],
      "metadata": {
        "id": "SXYBz_9CSEqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Define Training Arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"content/drive/MyDrive/asomsem\",  # Change to your desired output directory\n",
        "    per_device_train_batch_size=16,  # Adjust batch size based on your resources\n",
        "    gradient_accumulation_steps=1,  # Increase if you have memory constraints\n",
        "    gradient_checkpointing=True,  # Enable gradient checkpointing for memory efficiency\n",
        "    fp16=True,  # Enable mixed precision training if your hardware supports it\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=1000,  # Evaluate every 1000 steps\n",
        "    learning_rate=1e-5,  # Adjust learning rate as needed\n",
        "    weight_decay=0.0,\n",
        "    warmup_steps=500,\n",
        "    save_steps=1000,  # Save checkpoints every 1000 steps\n",
        "    logging_dir=\"./logs\",  # Directory for storing logs\n",
        "    num_train_epochs=3, # Adjust the number of training epochs\n",
        "    push_to_hub=False, # Set to True if you want to push your model to Hugging Face Hub\n",
        "    report_to=\"none\"\n",
        ")"
      ],
      "metadata": {
        "id": "VDMZjOQwSG2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Define Trainer and Train\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dsnew[\"train\"],\n",
        "    eval_dataset=dsnew[\"validation\"],\n",
        "    data_collator=data_collator\n",
        "    #report_to = \"none\"\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# 5. Save the Model\n",
        "trainer.save_model(\"content/drive/MyDrive/asomsem\")  # Save the trained model\n"
      ],
      "metadata": {
        "id": "n8q_d_reLY6b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}