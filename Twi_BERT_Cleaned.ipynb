{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Environment setup and installations"
      ],
      "metadata": {
        "id": "iLoOOtJN01LK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/openai/whisper.git"
      ],
      "metadata": {
        "id": "FP9SwIMiP3Ss",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets tokenizers\n",
        "import io\n",
        "import os\n",
        "import zipfile\n",
        "import requests\n",
        "import pandas as pd\n",
        "import torch\n",
        "import whisper\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM, pipeline\n",
        "# Load the BERT model and tokenizer for corrections\n",
        "from transformers import BertTokenizer, BertForMaskedLM, pipeline"
      ],
      "metadata": {
        "id": "TqPPie2X08MH",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "AjU2v_ABiPHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading data"
      ],
      "metadata": {
        "id": "59wlFVMWb8BZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AsantiTwiDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, zip_url, csv_filename, audio_base_path, processor=None, device=\"cpu\"):\n",
        "        self.device = device\n",
        "        self.audio_base_path = audio_base_path\n",
        "        self.processor = processor\n",
        "\n",
        "        # Download and extract the dataset\n",
        "        response = requests.get(zip_url, stream=True)\n",
        "        response.raise_for_status()\n",
        "        with zipfile.ZipFile(io.BytesIO(response.content), 'r') as zip_ref:\n",
        "            zip_ref.extractall('.')\n",
        "\n",
        "        # Clean the CSV file\n",
        "        cleaned_csv_filename = f\"cleaned_{os.path.basename(csv_filename)}\"\n",
        "        self.clean_csv(csv_filename, cleaned_csv_filename)\n",
        "\n",
        "        # Load the cleaned CSV\n",
        "        self.df = pd.read_csv(cleaned_csv_filename)\n",
        "\n",
        "        # Map columns if needed\n",
        "        column_mapping = {\n",
        "            \"Audio Filepath\": \"path\",\n",
        "            \"Transcription\": \"sentence\",\n",
        "        }\n",
        "        self.df.rename(columns=lambda x: column_mapping.get(x.strip(), x.strip()), inplace=True)\n",
        "\n",
        "        # Verify required columns\n",
        "        if 'path' not in self.df.columns or 'sentence' not in self.df.columns:\n",
        "            raise ValueError(\"CSV file must contain 'path' and 'sentence' columns.\")\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Returns the total number of samples in the dataset.\"\"\"\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"Returns a single data item at a given index.\"\"\"\n",
        "        # Get the row from the DataFrame\n",
        "        row = self.df.iloc[index]\n",
        "\n",
        "        # Construct the audio file path\n",
        "        audio_path = os.path.join(self.audio_base_path, row['path'])\n",
        "\n",
        "        # Get the corresponding sentence\n",
        "        sentence = row['sentence']\n",
        "\n",
        "        # Return a dictionary containing the audio path and sentence\n",
        "        return {'audio_path': audio_path, 'sentence': sentence}\n",
        "\n",
        "    @staticmethod\n",
        "    def clean_csv(input_path, output_path):\n",
        "        \"\"\"\n",
        "        Cleans a CSV file by:\n",
        "        - Replacing tab characters with commas.\n",
        "        - Filtering rows with inconsistent numbers of fields.\n",
        "        \"\"\"\n",
        "        with open(input_path, \"r\") as infile:\n",
        "            lines = infile.readlines()\n",
        "\n",
        "        # Replace tabs with commas\n",
        "        clean_lines = [line.replace(\"\\t\", \",\").replace(\"lacuna-audios-train/asanti-twi/audios/\", \"\").replace(\"lacuna-audios-test/asanti-twi/audios/\", \"\") for line in lines]\n",
        "\n",
        "        # Filter rows with the correct number of fields\n",
        "        expected_fields = clean_lines[0].count(\",\") + 1\n",
        "        valid_lines = [line for line in clean_lines if line.count(\",\") + 1 == expected_fields]\n",
        "\n",
        "        # Write cleaned content to a new file\n",
        "        with open(output_path, \"w\") as outfile:\n",
        "            outfile.writelines(valid_lines)\n",
        "\n",
        "\n",
        "# Dataset URLs and paths\n",
        "train_zip_url = \"https://fisd-dataset.s3.amazonaws.com/fisd-asanti-twi-90p.zip\"\n",
        "train_csv_filename = \"fisd-asanti-twi-90p/data.csv\"\n",
        "train_audio_base_path = \"fisd-asanti-twi-90p/audios\"\n",
        "\n",
        "test_zip_url = \"https://fisd-dataset.s3.amazonaws.com/fisd-asanti-twi-10p.zip\"\n",
        "test_csv_filename = \"fisd-asanti-twi-10p/data.csv\"\n",
        "test_audio_base_path = \"fisd-asanti-twi-10p/audios\"\n",
        "\n",
        "# Load datasets\n",
        "test_dataset = AsantiTwiDataset(test_zip_url, test_csv_filename, test_audio_base_path)\n",
        "train_dataset = AsantiTwiDataset(train_zip_url, train_csv_filename, train_audio_base_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "j7j2v6_9srWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testinglabels = \"/content/drive/MyDrive/ABENA_Trained/Testlabels.csv\"\n",
        "df = pd.read_csv(testinglabels, delimiter=\"\\t\", names=[\"Index\", \"path\",\"sentence\",\"translation\"], header=0)\n",
        "display(df)"
      ],
      "metadata": {
        "id": "5RGGX-dmtMeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions to use BERT Model"
      ],
      "metadata": {
        "id": "YsXzfRBSb_3E"
      }
    },
    {
      "source": [
        "import transformers.pipelines.pt_utils\n",
        "def correct_asr_output(text):\n",
        "\n",
        "    masked_text = text + \" [MASK]\"  # Add the mask token at the end\n",
        "    corrected_text = corrector_robako(masked_text)\n",
        "    corrected_word = corrected_text[0]['token_str']\n",
        "    corrected_sentence = masked_text.replace(\"[MASK]\", corrected_word)\n",
        "\n",
        "    corrected_sequence = corrected_sentence.replace(\"[CLS]\", \"\").replace(\"[SEP]\", \"\").strip()\n",
        "    # Extract the predicted token (ignoring the original input and [MASK])\n",
        "    return corrected_sequence[0][0]\n",
        "\n",
        "\n",
        "def transcribe_audio(audio_path):\n",
        "    # Pass options as keyword arguments within transcribe\n",
        "    model = whisper.load_model(\"tiny\")\n",
        "    result = model.transcribe(audio_path, language=\"en\", without_timestamps=True)\n",
        "    return result['text']"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "J7-Thl5Dyvhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Example usage for selecting a random statement\n",
        "import random\n",
        "import os\n",
        "\n",
        "\n",
        "# Select a random index within the dataset\n",
        "random_index = random.randint(0, len(test_dataset) - 1)\n",
        "\n",
        "# Get the data item at the random index\n",
        "random_item = test_dataset[random_index]\n",
        "\n",
        "# Access the audio path and sentence from the random item\n",
        "audio_path = random_item['audio_path']\n",
        "sentence = random_item['sentence']\n",
        "\n",
        "print(\"Random Audio Path:\", audio_path)\n",
        "print(\"Random Sentence:\", sentence)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "warnings.filterwarnings(\"ignore\", message=\"Some weights of the model checkpoint\") #Was flooding the output\n",
        "\n",
        "# Transcribe and correct the random audio\n",
        "asr_output = transcribe_audio(audio_path)\n",
        "print(\"ASR Output:\", asr_output)\n",
        "\n",
        "# Load the BERT Abena model for correction\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM, pipeline\n",
        "\n",
        "tokenizer_abena = AutoTokenizer.from_pretrained(\"Ghana-NLP/abena-base-asante-twi-uncased\")\n",
        "corrector_model_abena = AutoModelForMaskedLM.from_pretrained(\"Ghana-NLP/abena-base-asante-twi-uncased\")\n",
        "corrector_abena = pipeline(\"fill-mask\", model=corrector_model_abena, tokenizer=tokenizer_abena, device=0)\n",
        "\n",
        "# Correct ASR output using Abena\n",
        "corrected_output_abena = corrector_abena(asr_output.replace(\" \", \" [MASK] \"))\n",
        "print(\"Corrected Output (Abena):\", corrected_output_abena)\n",
        "print(\"\\n\\n\\n\")\n",
        "\n",
        "# Load the BERT Robako model for correction\n",
        "tokenizer_robako = AutoTokenizer.from_pretrained(\"Ghana-NLP/robako-base-asante-twi-uncased\")\n",
        "corrector_model_robako = AutoModelForMaskedLM.from_pretrained(\"Ghana-NLP/robako-base-asante-twi-uncased\")\n",
        "corrector_robako = pipeline(task = \"fill-mask\", model=corrector_model_robako, tokenizer=tokenizer_robako, device=0)\n",
        "\n",
        "# Correct ASR output using Robako\n",
        "#loads of errors here while trying to extract just the sentence, so tried different ways to get just the output\n",
        "#masked_texts = [correct_asr_output(text) for text in asr_output.split()]\n",
        "#corrected_output_robako = [corrector_robako(text)[0]['token_str'] for text in asr_output.split()]\n",
        "#corrected_output_robako = \" \".join(corrected_output_robako)\n",
        "#corrected_output_robako = [corrector_robako(text + \" [MASK]\")[0]['token_str'] for text in asr_output.split()]\n",
        "corrected_output_robako = [corrector_robako(text + \" <mask>\")[0]['sequence'] for text in asr_output.split()]\n",
        "corrected_output_robako = \" \".join(corrected_output_robako)\n",
        "print(\"Corrected Output (Robako):\", corrected_output_robako)\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "0XtMzbqF413L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Using BERT to correct errors"
      ],
      "metadata": {
        "id": "jsMPfOSQMSBV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgyCcyr6m3-o"
      },
      "source": [
        "## Method 1: Language Modelling\n",
        "Trying to use predicting the next work to correct it (later figured out it was not a meaningful way to get things done)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egak1_OZnCkt"
      },
      "outputs": [],
      "source": [
        "#for language modelling\n",
        "\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import BertTokenizer, BertForMaskedLM, Trainer, TrainingArguments\n",
        "\n",
        "\n",
        "dataset = Dataset.from_pandas(train_dataset.df)\n",
        "#dataset = Dataset.from_pandas(test_dataset)\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"Ghana-NLP/abena-base-asante-twi-uncased\")\n",
        "\n",
        "# Tokenize the dataset\n",
        "'''def tokenize_function(examples):\n",
        "    # Specifying max_length here ensures all sequences are the same length\n",
        "    return tokenizer(examples[\"sentence\"], padding=\"max_length\", truncation=True, max_length=128) # Added max_length'''\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    # Specifying max_length here ensures all sequences are the same length\n",
        "    # Create labels by replacing masked tokens with -100 (ignore index)\n",
        "    inputs = tokenizer(examples[\"sentence\"], padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")  # Added max_length, return_tensors=\"pt\"\n",
        "    inputs[\"labels\"] = inputs.input_ids.detach().clone()\n",
        "    # create random array of floats with equal dims to input_ids\n",
        "    rand = torch.rand(inputs.input_ids.shape)\n",
        "    # mask random 15% where token is not 0 [PAD], 1 [CLS], or 2 [SEP]\n",
        "    mask_arr = (rand < 0.15) * (inputs.input_ids != 0) * (inputs.input_ids != 1) * (inputs.input_ids != 2)\n",
        "    # loop through each row in input_ids tensor (cannot do in parallel)\n",
        "    for i in range(inputs.input_ids.shape[0]):\n",
        "        # get indices of mask positions from mask array\n",
        "        selection = torch.flatten(mask_arr[i].nonzero()).tolist()\n",
        "        # mask input_ids\n",
        "        inputs.input_ids[i, selection] = tokenizer.mask_token_id\n",
        "    # where input_ids is not masked, set labels to -100\n",
        "    inputs[\"labels\"][~mask_arr] = -100\n",
        "\n",
        "    return inputs\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "tokenized_datasets = tokenized_datasets.train_test_split(test_size=0.2)\n",
        "\n",
        "# Load the model\n",
        "model = BertForMaskedLM.from_pretrained(\"Ghana-NLP/abena-base-asante-twi-uncased\")\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=6,\n",
        "    weight_decay=0.01,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "trainer.save_model(\"/content/drive/MyDrive/ABENA_Trained2\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/ABENA_Trained2\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"/content/drive/MyDrive/ABENA_Trained\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/ABENA_Trained\")"
      ],
      "metadata": {
        "id": "qHA3CIsgicxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertForMaskedLM, pipeline\n",
        "\n",
        "# Load your fine-tuned model and tokenizer\n",
        "model_path = \"/content/drive/MyDrive/ABENA_Trained\"  # Update with your save path\n",
        "\n",
        "# Load the tokenizer from the configuration file\n",
        "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
        "\n",
        "# Load the model, potentially specifying the safetensors file\n",
        "model = BertForMaskedLM.from_pretrained(model_path, torch_dtype=\"auto\")\n",
        "\n",
        "# Create a fill-mask pipeline\n",
        "fill_mask = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Test with a sentence\n",
        "test_sentence = \"Mɛtumi ayɛ [MASK] million cedis.\"\n",
        "predictions = fill_mask(test_sentence)\n",
        "\n",
        "# Print the predictions\n",
        "for prediction in predictions:\n",
        "    print(f\"Predicted word: {prediction['token_str']}, Score: {prediction['score']:.4f}\")\n",
        "\n",
        "\n",
        "# Print only the predicted word\n",
        "predicted_word = predictions[0]['token_str']\n",
        "print(f\"Predicted sentence: {test_sentence.replace('[MASK]', predicted_word)}\")"
      ],
      "metadata": {
        "id": "o0cN8aFOkHnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "def mask_random_word(sentence):\n",
        "    \"\"\"Masks a single random word in a sentence.\n",
        "\n",
        "    Args:\n",
        "        sentence (str): The input sentence.\n",
        "\n",
        "    Returns:\n",
        "        str: The sentence with one word randomly masked.\n",
        "    \"\"\"\n",
        "    words = sentence.split()\n",
        "    if words:  # Check if the sentence is not empty\n",
        "        random_index = random.randint(0, len(words) - 1)  # Get a random index\n",
        "        words[random_index] = \"[MASK]\"  # Replace the word at that index\n",
        "    return \" \".join(words)  # Join the words back into a sentence"
      ],
      "metadata": {
        "id": "dPPsrrVCvleL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd # If using a CSV file\n",
        "from transformers import BertTokenizer, BertForMaskedLM, pipeline\n",
        "\n",
        "# Load fine-tuned model and tokenizer\n",
        "model_path = \"/content/drive/MyDrive/ABENA_Trained\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
        "model = BertForMaskedLM.from_pretrained(model_path)\n",
        "fill_mask = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "\n",
        "testinglabels = \"/content/drive/MyDrive/ABENA_Trained2/Testlabels.csv\"\n",
        "df = pd.read_csv(testinglabels, delimiter=\"\\t\", names=[\"Index\", \"path\",\"sentence\",\"translation\"], header=0)\n",
        "df[\"masked_sentence\"] = df[\"sentence\"].apply(mask_random_word)\n",
        "\n",
        "\n",
        "# Prediction loop\n",
        "#for sentence in sentences:\n",
        "for index, row in df.iterrows():\n",
        "    # Replace a word with [MASK] for prediction\n",
        "    masked_sentence = row[\"masked_sentence\"]\n",
        "    predictions = fill_mask(masked_sentence)\n",
        "    #masked_sentence = sentence.replace(\"target_word\", \"[MASK]\")\n",
        "    #predictions = fill_mask(masked_sentence)\n",
        "    predicted_word = predictions[0]['token_str']  # Get the top prediction\n",
        "\n",
        "    # Compare predicted word with actual word\n",
        "    print(f\"Original Sentence: {row['sentence']}\")\n",
        "    print(f\"Masked Sentence: {masked_sentence}\")\n",
        "    print(f\"Predicted Word: {predicted_word}\")\n",
        "    print(f\"Predicted Sentence: {masked_sentence.replace('[MASK]', predicted_word)}\")\n",
        "    print(\"-\" * 20)  # Separator"
      ],
      "metadata": {
        "id": "BYlXzdD5rE7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Method 2: Text Correction\n",
        "This is the actual way to do correct the Whisper output, but had insufficient data for it. Was using the error data rom the finetuning of the whisper model on the financial inclusion dataset."
      ],
      "metadata": {
        "id": "kje-9Y6qiqDG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zR3LH6FMmxnP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "from transformers import BertTokenizer, EncoderDecoderModel, Trainer, TrainingArguments\n",
        "\n",
        "\n",
        "from transformers import BertTokenizer, BertForMaskedLM, pipeline\n",
        "\n",
        "# Load your fine-tuned model and tokenizer\n",
        "model_path = \"/content/drive/MyDrive/ABENA_Trained\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
        "model = BertForMaskedLM.from_pretrained(model_path)\n",
        "fill_mask = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtZcLmzqmyOp"
      },
      "outputs": [],
      "source": [
        "testinglabels = \"/content/drive/MyDrive/ABENA_Trained/hypotheses_references.csv\"\n",
        "dataset = pd.read_csv(testinglabels, names=[\"Index\", \"path\",\"sentence\",\"translation\"], header=0)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets transformers\n",
        "\n",
        "\n",
        "# Load a pre-trained BERT model for Asante Twi\n",
        "model_name = \"Ghana-NLP/abena-base-asante-twi-uncased\"  # Or \"Ghana-NLP/robako-base-asante-twi-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForMaskedLM.from_pretrained(model_name)\n",
        "\n",
        "fill_mask = pipeline(\"fill-mask\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "# Example test sentence\n",
        "test_sentence = \"Mɛtumi ayɛ [MASK] million cedis.\"\n",
        "predictions = fill_mask(test_sentence)\n",
        "print(f\"Predicted sentence: {test_sentence.replace('[MASK]', predictions[0]['token_str'])}\")"
      ],
      "metadata": {
        "id": "8KEA03BSj9Ac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "data_path = \"/content/drive/MyDrive/ABENA_Trained/hypotheses_references.csv\"\n",
        "df = pd.read_csv(data_path, names=[\"incorrect\", \"correct\", \"incorrect cleaned\", \"correct cleaned\"], header=0)\n",
        "dataset = Dataset.from_pandas(df)\n",
        "\n",
        "# Tokenization function\n",
        "def preprocess_function(examples):\n",
        "    # Use the 'incorrect cleaned' and 'correct cleaned' columns\n",
        "    inputs = tokenizer(examples[\"incorrect cleaned\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(examples[\"correct cleaned\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return inputs\n",
        "\n",
        "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# Split into train, validation, and test sets\n",
        "from datasets import DatasetDict # Import DatasetDict here\n",
        "train_testvalid = tokenized_datasets.train_test_split(test_size=0.2)\n",
        "test_valid = train_testvalid['test'].train_test_split(test_size=0.5)\n",
        "tokenized_datasets = DatasetDict({\n",
        "    'train': train_testvalid['train'],\n",
        "    'test': test_valid['test'],\n",
        "    'validation': test_valid['train']})\n",
        "\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit = 2, # Only last 2 models are saved. Older ones are deleted.\n",
        "    load_best_model_at_end=True,\n",
        "    save_strategy=\"epoch\",\n",
        "    report_to=\"none\",\n",
        "    logging_steps = 10\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"]\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the fine-tuned model\n",
        "trainer.save_model(\"/content/drive/MyDrive/ABENA_FineTuned\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/ABENA_FineTuned\")"
      ],
      "metadata": {
        "id": "W2JWE4RQkB_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM, pipeline\n",
        "\n",
        "# Load the fine-tuned model\n",
        "fine_tuned_tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/MyDrive/ABENA_FineTuned\")\n",
        "fine_tuned_model = AutoModelForMaskedLM.from_pretrained(\"/content/drive/MyDrive/ABENA_FineTuned\")\n",
        "\n",
        "corrector = pipeline(\"fill-mask\", model=fine_tuned_model, tokenizer=fine_tuned_tokenizer)\n",
        "\n",
        "def correct_text(text):\n",
        "    \"\"\"Corrects the input text using the fine-tuned model.\"\"\"\n",
        "    # Tokenize the input text\n",
        "    inputs = fine_tuned_tokenizer(text, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"][0]  # Access the first element of the tensor\n",
        "\n",
        "    # Create a copy of the input_ids to avoid modifying the original\n",
        "    masked_input_ids = input_ids.clone()\n",
        "\n",
        "    # Randomly mask 15% of the tokens (excluding special tokens)\n",
        "    rand = torch.rand(input_ids.shape)\n",
        "    mask_arr = (rand < 0.15) * (input_ids != fine_tuned_tokenizer.cls_token_id) * (input_ids != fine_tuned_tokenizer.sep_token_id)\n",
        "\n",
        "    # Apply the mask to the selected tokens\n",
        "    masked_input_ids[mask_arr] = fine_tuned_tokenizer.mask_token_id\n",
        "\n",
        "    # Run the model to predict the masked tokens\n",
        "    outputs = fine_tuned_model(masked_input_ids.unsqueeze(0))  # Add batch dimension\n",
        "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "\n",
        "    # Decode the predicted tokens to get the corrected text\n",
        "    corrected_text = fine_tuned_tokenizer.decode(predictions[0], skip_special_tokens=True)\n",
        "\n",
        "    return corrected_text\n"
      ],
      "metadata": {
        "id": "KiQ-L6SqkGu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "text_to_correct = \"Me den de Kwoku\"\n",
        "corrected_text = correct_text(text_to_correct)\n",
        "print(f\"Original Text: {text_to_correct}\")\n",
        "print(f\"Corrected Text: {corrected_text}\")"
      ],
      "metadata": {
        "id": "Z_VxrUIwuudr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}